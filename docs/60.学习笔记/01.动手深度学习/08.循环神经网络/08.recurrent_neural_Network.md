---
title: 循环神经网络
date: 2024-04-25 09:51:03
permalink: /learn/d2l/471b4f/
categories:
  - 学习笔记
  - 动手深度学习
tags:
  - RNN
feed:
  enable: true
---

开始了文本方向的神经网络，之前的卷积看完还是很不知所云的， 对于为什么这样会更好还是只有一个大概的解释，估计文本这方面也不遑多让吧。

I have started learing language Models, but I'm still confused after studying Convolutional Neural Networks. The explanations provided are quite vague, and I suspect Language Models might be just as challenging as CNNs

<!-- more -->

这一节从马可夫链开始，到一个简单的 RNN 实现结束，主要的难点还是在 RNN 实现方面的细节。

This section begins from Markov Chain, and concludes with a basic implementation of an RNN from scratch. The main challenge lies in the detailed of implementation of the RNN.

## 循环神经网络从零开始实现 / rnn scratch

### 数据格式 / data format

```python
train_iter, vocab = load_data_time_machine(batch_size, num_steps)
```

:::note
train_iter: 每次输出一个批量大小的 X 和 Y 。

train_iter: ouput a batch of X and Y with each iteration

X:  tensor([[13, 14, 15, 16, 17],
        [28, 29, 30, 31, 32]])

Y: tensor([[14, 15, 16, 17, 18],
        [29, 30, 31, 32, 33]])

X:  tensor([[ 3,  4,  5,  6,  7],
        [18, 19, 20, 21, 22]])

Y: tensor([[ 4,  5,  6,  7,  8],
        [19, 20, 21, 22, 23]])

X:  tensor([[ 8,  9, 10, 11, 12],
[23, 24, 25, 26, 27]])

Y: tensor([[ 9, 10, 11, 12, 13],
        [24, 25, 26, 27, 28]])
:::

### 为什么要对 X 进行转置 / why should we transpose tensor X

首先需要知道的是本节所有的预测都是对<mark>字符</mark>的预测，所以 one_hot 的分类数是 28 。

Firstly, it is important to note that all predictions in this section are predictions of characters, so the number of class for the one-hot encoding is 28.

:::note
26 个英文字母 + 空格 + unk

26 English characters + space + unk
:::

```python
import torch
from torch.nn import functional as F

X = torch.arange(10).reshape(2, 5)
print(X)
ans = F.one_hot(X, 28)
print(ans)
print(ans.shape)
```

每次做的是对一个批量进行预测，所以每次我们取的是

Since our predictions are conducted on batches, our selection  contain

$$\begin{bmatrix}
0&1\\
1&2\\
2&3\\
\end{bmatrix}$$

这里将X进行转置后相邻的批次读取的是<mark>时间</mark>上连续的，也就是文本顺序是正确的。

We transpose X to ensure that consecutive batches are read in time order, meaning the text sequence is pre preserved

### 转置的好处 / the benefits of transposition

其实就是一个行优先和列优先的问题，以及个人的习惯。

this a column provity or row proviet, and personal habbits
this is actually a question of row priority versus column priority, as well as individual habits

python 是行优先的，所以每次批量读取转值后会带来 cache 命中的提升。

Python follows row priority, so each batch transpositon will result in improvement in cache hit rate

## torch.mm

还是看GPT吧

torch.mm() 和 torch.matmul() 是 PyTorch 中用于执行矩阵乘法的两个函数，它们有一些区别：

### 输入类型：

torch.mm() 只能接受二维张量作为输入，即矩阵。

torch.matmul() 可以接受张量的任意维度作为输入，因此可以用于更广泛的矩阵乘法操作，包括批量矩阵乘法、广播等。

### 广播规则：

torch.mm() 对输入张量进行严格的形状匹配，要求两个输入张量的形状都是二维，并且第一个张量的列数必须等于第二个张量的行数。

torch.matmul() 则遵循广播规则，可以在满足一定条件的情况下，对具有不同形状的张量进行乘法操作。例如，可以对两个三维张量进行乘法，其中第一个张量的最后两个维度的形状必须与第二个张量的倒数两个维度的形状相匹配。

### 支持批量矩阵乘法：

torch.mm() 不支持批量矩阵乘法，即一次性处理多个矩阵乘积。

torch.matmul() 可以通过在输入张量的前面添加额外的维度来支持批量矩阵乘法，这在处理多个样本或批次数据时非常有用。

## 有关 Y/label

Y是一个 2x5 的 tensor ， rnn输出的是一个 10x28 的 tensor，这也就意味着 loss 那里要做一个 reshape。

### rnn 细节

output 是一个 len 为 5 的 list 。list 里面为 torch.size(2,28) 的tensor。

对 list 进行 cat dim = 0 。
